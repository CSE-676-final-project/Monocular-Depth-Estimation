{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import Depthset, Agument, toTensor\n",
    "from models import Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import mat73\n",
    "import kornia\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:ERROR: MATLAB type not supported: containers.Map, (uint32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./data/nyu_depth_v2_labeled.mat\"\n",
    "data_dict = mat73.loadmat(file_path)\n",
    "\n",
    "dataset = Depthset(\n",
    "    data_dict=data_dict,\n",
    "    transform=transforms.Compose([Agument(probability=0.5),toTensor()])\n",
    "    )\n",
    "print(\"dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50] loss: 545.1849279403687 time: 173.84489917755127\n",
      "[1/50] loss: 352.62832152843475 time: 172.79069185256958\n",
      "[2/50] loss: 308.8652274608612 time: 195.86176943778992\n",
      "[3/50] loss: 298.69083750247955 time: 205.3348367214203\n",
      "[4/50] loss: 290.95823788642883 time: 228.58640360832214\n",
      "[5/50] loss: 294.1490936279297 time: 214.72597098350525\n",
      "[6/50] loss: 272.1088819503784 time: 211.47890615463257\n",
      "[7/50] loss: 257.3126937150955 time: 218.44366455078125\n",
      "[8/50] loss: 280.8701751232147 time: 216.91672110557556\n",
      "[9/50] loss: 264.2503002882004 time: 220.77834749221802\n",
      "[10/50] loss: 267.62588942050934 time: 197.20578360557556\n",
      "[11/50] loss: 246.6959948539734 time: 194.70586800575256\n",
      "[12/50] loss: 241.21664303541183 time: 194.14415431022644\n",
      "[13/50] loss: 252.8209490776062 time: 193.80414962768555\n",
      "[14/50] loss: 248.99874424934387 time: 234.64682245254517\n",
      "[15/50] loss: 235.296340405941 time: 227.24001812934875\n",
      "[16/50] loss: 239.5244083404541 time: 243.10043668746948\n",
      "[17/50] loss: 241.91922575235367 time: 228.10708689689636\n",
      "[18/50] loss: 237.36457079648972 time: 222.52836203575134\n",
      "[19/50] loss: 225.45422035455704 time: 222.66197276115417\n",
      "[20/50] loss: 225.44486886262894 time: 241.27628111839294\n",
      "[21/50] loss: 218.97840893268585 time: 242.08806133270264\n",
      "[22/50] loss: 214.93269300460815 time: 248.1030523777008\n",
      "[23/50] loss: 229.64771258831024 time: 239.30095267295837\n",
      "[24/50] loss: 229.79672318696976 time: 242.7709743976593\n",
      "[25/50] loss: 215.91129380464554 time: 230.79701113700867\n",
      "[26/50] loss: 213.1971396803856 time: 224.2920196056366\n",
      "[27/50] loss: 210.32084357738495 time: 231.9666292667389\n",
      "[28/50] loss: 204.24479168653488 time: 194.26274037361145\n",
      "[29/50] loss: 199.6505160331726 time: 215.54281520843506\n",
      "[30/50] loss: 195.28272032737732 time: 242.01993131637573\n",
      "[31/50] loss: 205.95356440544128 time: 1429.0782668590546\n",
      "[32/50] loss: 200.80577141046524 time: 285.69250988960266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\naine\\Documents\\github\\Monocular-Depth-Estimation\\train.ipynb Cell 3'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Monocular-Depth-Estimation/train.ipynb#ch0000005?line=44'>45</a>\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Monocular-Depth-Estimation/train.ipynb#ch0000005?line=46'>47</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Monocular-Depth-Estimation/train.ipynb#ch0000005?line=47'>48</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Monocular-Depth-Estimation/train.ipynb#ch0000005?line=49'>50</a>\u001b[0m loss_epoch\u001b[39m.\u001b[39mappend(cum_loss\u001b[39m/\u001b[39mcounter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/naine/Documents/github/Monocular-Depth-Estimation/train.ipynb#ch0000005?line=51'>52</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m./trained_models/loss_log\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fp:\n",
      "File \u001b[1;32mc:\\Users\\naine\\def\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/optimizer.py?line=85'>86</a>\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/optimizer.py?line=86'>87</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/optimizer.py?line=87'>88</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\naine\\def\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/autograd/grad_mode.py?line=23'>24</a>\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/autograd/grad_mode.py?line=24'>25</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/autograd/grad_mode.py?line=25'>26</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/autograd/grad_mode.py?line=26'>27</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\naine\\def\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=137'>138</a>\u001b[0m             \u001b[39m# record the step after step update\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=138'>139</a>\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=140'>141</a>\u001b[0m     F\u001b[39m.\u001b[39;49madam(params_with_grad,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=141'>142</a>\u001b[0m            grads,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=142'>143</a>\u001b[0m            exp_avgs,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=143'>144</a>\u001b[0m            exp_avg_sqs,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=144'>145</a>\u001b[0m            max_exp_avg_sqs,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=145'>146</a>\u001b[0m            state_steps,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=146'>147</a>\u001b[0m            amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=147'>148</a>\u001b[0m            beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=148'>149</a>\u001b[0m            beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=149'>150</a>\u001b[0m            lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=150'>151</a>\u001b[0m            weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=151'>152</a>\u001b[0m            eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=152'>153</a>\u001b[0m            maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/adam.py?line=153'>154</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\naine\\def\\lib\\site-packages\\torch\\optim\\_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=93'>94</a>\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=95'>96</a>\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=96'>97</a>\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1)\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=97'>98</a>\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m     <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=98'>99</a>\u001b[0m \u001b[39mif\u001b[39;00m amsgrad:\n\u001b[0;32m    <a href='file:///c%3A/Users/naine/def/lib/site-packages/torch/optim/_functional.py?line=99'>100</a>\u001b[0m     \u001b[39m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model().cuda()\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "batchsize = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batchsize,\n",
    "    shuffle=True\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "criterion = nn.L1Loss()\n",
    "ssim = kornia.losses.SSIMLoss(window_size = 11, max_val = 1000.0 / 10.0, reduction='none')\n",
    "\n",
    "losses = []\n",
    "loss_epoch = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cum_loss = 0\n",
    "    counter = 0\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        image = batch['image'].cuda()\n",
    "        depth = batch['depth'].cuda()\n",
    "\n",
    "        depth = 1000.0 / depth\n",
    "\n",
    "        out = model(image)\n",
    "\n",
    "        loss = criterion(out, depth)\n",
    "        l_ssim = torch.clamp((1 - ssim(out, depth)) * 0.5, 0, 1)\n",
    "        loss = (1.0 * l_ssim.mean().item()) + (0.1 * loss)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        cum_loss += loss.item()\n",
    "        counter += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_epoch.append(cum_loss/counter)\n",
    "\n",
    "    with open(\"./trained_models/loss_log\",'wb') as fp:\n",
    "        pickle.dump(losses,fp)\n",
    "\n",
    "    with open(\"./trained_models/epoch_loss_log\",'wb') as fp:\n",
    "        pickle.dump(loss_epoch,fp)\n",
    "\n",
    "    path = f\"./trained_models/{epoch}.pth\"\n",
    "    torch.save(model.state_dict(),path)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"[{epoch}/{epochs}] loss: {cum_loss} time: {end - start}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5b53d600c30c432c9f427e0099044907f5b90ab104622449e78aa45f8a63a20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('def': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
